{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ce5c025",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\conda\\envs\\clap_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import torch\n",
    "import laion_clap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8be058fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lenovo\\conda\\envs\\clap_env\\lib\\site-packages\\torch\\functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\TensorShape.cpp:3596.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = laion_clap.CLAP_Module(enable_fusion=False, amodel= 'HTSAT-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a9b325e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load the specified checkpoint ../model_checkpoint/music_audioset_epoch_15_esc_90.14 (1).pt from users.\n",
      "Load Checkpoint...\n",
      "logit_scale_a \t Loaded\n",
      "logit_scale_t \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_real.weight \t Loaded\n",
      "audio_branch.spectrogram_extractor.stft.conv_imag.weight \t Loaded\n",
      "audio_branch.logmel_extractor.melW \t Loaded\n",
      "audio_branch.bn0.weight \t Loaded\n",
      "audio_branch.bn0.bias \t Loaded\n",
      "audio_branch.patch_embed.proj.weight \t Loaded\n",
      "audio_branch.patch_embed.proj.bias \t Loaded\n",
      "audio_branch.patch_embed.norm.weight \t Loaded\n",
      "audio_branch.patch_embed.norm.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.0.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.0.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.0.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.1.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.1.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.1.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.2.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.3.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.4.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.5.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.6.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.6.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.6.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.7.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.7.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.7.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.8.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.8.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.8.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.9.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.9.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.9.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.10.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.10.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.10.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.norm1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.norm1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.2.blocks.11.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.norm2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.norm2.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.2.blocks.11.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.2.blocks.11.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.2.downsample.reduction.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.weight \t Loaded\n",
      "audio_branch.layers.2.downsample.norm.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.0.mlp.fc2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.relative_position_bias_table \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.qkv.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.attn.proj.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.norm2.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc1.bias \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.weight \t Loaded\n",
      "audio_branch.layers.3.blocks.1.mlp.fc2.bias \t Loaded\n",
      "audio_branch.norm.weight \t Loaded\n",
      "audio_branch.norm.bias \t Loaded\n",
      "audio_branch.tscam_conv.weight \t Loaded\n",
      "audio_branch.tscam_conv.bias \t Loaded\n",
      "audio_branch.head.weight \t Loaded\n",
      "audio_branch.head.bias \t Loaded\n",
      "text_branch.embeddings.word_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.position_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.token_type_embeddings.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.weight \t Loaded\n",
      "text_branch.embeddings.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.0.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.1.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.2.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.3.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.4.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.5.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.6.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.7.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.8.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.9.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.10.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.query.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.key.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.self.value.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.attention.output.LayerNorm.bias \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.intermediate.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.dense.bias \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.weight \t Loaded\n",
      "text_branch.encoder.layer.11.output.LayerNorm.bias \t Loaded\n",
      "text_branch.pooler.dense.weight \t Loaded\n",
      "text_branch.pooler.dense.bias \t Loaded\n",
      "text_transform.sequential.0.weight \t Loaded\n",
      "text_transform.sequential.0.bias \t Loaded\n",
      "text_transform.sequential.3.weight \t Loaded\n",
      "text_transform.sequential.3.bias \t Loaded\n",
      "text_projection.0.weight \t Loaded\n",
      "text_projection.0.bias \t Loaded\n",
      "text_projection.2.weight \t Loaded\n",
      "text_projection.2.bias \t Loaded\n",
      "audio_transform.sequential.0.weight \t Loaded\n",
      "audio_transform.sequential.0.bias \t Loaded\n",
      "audio_transform.sequential.3.weight \t Loaded\n",
      "audio_transform.sequential.3.bias \t Loaded\n",
      "audio_projection.0.weight \t Loaded\n",
      "audio_projection.0.bias \t Loaded\n",
      "audio_projection.2.weight \t Loaded\n",
      "audio_projection.2.bias \t Loaded\n"
     ]
    }
   ],
   "source": [
    "model.load_ckpt('../model_checkpoint/music_audioset_epoch_15_esc_90.14 (1).pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "678f974c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity vector: tensor([0.3252], device='cuda:0', grad_fn=<SumBackward1>)\n",
      "First similarity (scalar): 0.3251603841781616\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def to_torch_tensor(x, device=None, dtype=torch.float32):\n",
    "    \"\"\"Convert numpy/torch/sequence to torch.Tensor on `device` with `dtype`.\n",
    "       Ensures a batch dimension (N, D).\n",
    "    \"\"\"\n",
    "    # detect numpy\n",
    "    if isinstance(x, np.ndarray):\n",
    "        t = torch.from_numpy(x)\n",
    "    elif isinstance(x, torch.Tensor):\n",
    "        t = x\n",
    "    else:\n",
    "        # if it's a list or scalar\n",
    "        t = torch.tensor(x)\n",
    "    # ensure float dtype\n",
    "    if not torch.is_floating_point(t):\n",
    "        t = t.float()\n",
    "    # ensure batch dim\n",
    "    if t.ndim == 1:\n",
    "        t = t.unsqueeze(0)   # make (1, D)\n",
    "    elif t.ndim == 0:\n",
    "        t = t.unsqueeze(0).unsqueeze(0)\n",
    "    # move to device if given\n",
    "    if device is not None:\n",
    "        t = t.to(device=device, dtype=dtype)\n",
    "    else:\n",
    "        t = t.to(dtype=dtype)\n",
    "    return t\n",
    "\n",
    "# choose device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# get embeddings (your current API calls)\n",
    "audio_embed = model.get_audio_embedding_from_filelist(\n",
    "    x=[\"../Main_DataSet_Creation/data/audio/00/16000.mp3\"],\n",
    "    use_tensor=True\n",
    ")\n",
    "text_embed = model.get_text_embedding([\n",
    "    \"Rock \"])\n",
    "\n",
    "# Convert both to torch tensors (handle numpy or torch returns)\n",
    "audio_t = to_torch_tensor(audio_embed, device=device)\n",
    "text_t  = to_torch_tensor(text_embed, device=device)\n",
    "\n",
    "# Optional safety: if shapes differ in dimension size, print helpful info\n",
    "if audio_t.shape[1] != text_t.shape[1]:\n",
    "    raise RuntimeError(f\"Embedding dimension mismatch: audio {audio_t.shape} vs text {text_t.shape}\")\n",
    "\n",
    "# Normalize (L2) each row so cosine = dot product\n",
    "audio_n = F.normalize(audio_t, p=2, dim=1)\n",
    "text_n  = F.normalize(text_t,  p=2, dim=1)\n",
    "\n",
    "# If both are batches you want pairwise similarity for each row:\n",
    "# - if both batches same length: per-row similarity\n",
    "# - if audio batch and single text row: compute each audio vs that text\n",
    "if audio_n.shape[0] == text_n.shape[0]:\n",
    "    sim = F.cosine_similarity(audio_n, text_n, dim=1)  # shape (N,)\n",
    "else:\n",
    "    # broadcast: compare each audio row to first text row\n",
    "    sim = F.cosine_similarity(audio_n, text_n.expand(audio_n.shape[0], -1), dim=1)\n",
    "\n",
    "print(\"Similarity vector:\", sim)\n",
    "print(\"First similarity (scalar):\", float(sim[0].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68eddb08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "def to_torch_tensor(x, device=None, dtype=torch.float32):\n",
    "    \"\"\"Convert numpy or torch to torch tensor on device with float dtype.\"\"\"\n",
    "    if isinstance(x, np.ndarray):\n",
    "        t = torch.from_numpy(x)\n",
    "    elif isinstance(x, torch.Tensor):\n",
    "        t = x\n",
    "    else:\n",
    "        raise TypeError(f\"Unsupported type {type(x)}\")\n",
    "    if t.dtype in (torch.float64, torch.float16):\n",
    "        t = t.to(dtype)\n",
    "    if device is not None:\n",
    "        t = t.to(device)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8bc7634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_normalize(t: torch.Tensor, eps: float = 1e-12) -> torch.Tensor:\n",
    "    \"\"\"L2-normalize along last dim.\"\"\"\n",
    "    denom = t.norm(p=2, dim=-1, keepdim=True).clamp_min(eps)\n",
    "    return t / denom\n",
    "\n",
    "def ensure_2d(t: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Make sure tensor is 2D: (N, D)\"\"\"\n",
    "    if t.ndim == 1:\n",
    "        return t.unsqueeze(0)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "232f3df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_audio_single(model, audio_path: str, device: torch.device):\n",
    "    \"\"\"Return audio embedding as torch tensor (1, D). Handles numpy or torch returns.\"\"\"\n",
    "    emb = model.get_audio_embedding_from_filelist(x=[audio_path], use_tensor=True)\n",
    "    # CLAP sometimes returns numpy array or torch; handle both\n",
    "    if isinstance(emb, np.ndarray):\n",
    "        emb_t = to_torch_tensor(emb, device=device)\n",
    "    else:\n",
    "        emb_t = emb.to(device)\n",
    "    emb_t = ensure_2d(emb_t.float())  # (1, D)\n",
    "    return emb_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7492a7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_texts(model, texts: List[str], device: torch.device, chunk_size:int = 256):\n",
    "    \"\"\"\n",
    "    Embed a list of texts. CLAP's get_text_embedding often accepts list directly.\n",
    "    This handles returns that might be numpy arrays and chunks if the list is large.\n",
    "    Returns (N, D) torch tensor on device.\n",
    "    \"\"\"\n",
    "    # try directly first\n",
    "    try:\n",
    "        te = model.get_text_embedding(texts)\n",
    "        if isinstance(te, np.ndarray):\n",
    "            te_t = to_torch_tensor(te, device=device)\n",
    "        else:\n",
    "            te_t = te.to(device)\n",
    "        return ensure_2d(te_t.float())\n",
    "    except Exception:\n",
    "        # chunked fallback (safer if the model method fails for huge lists)\n",
    "        out = []\n",
    "        for i in range(0, len(texts), chunk_size):\n",
    "            chunk = texts[i:i+chunk_size]\n",
    "            te = model.get_text_embedding(chunk)\n",
    "            if isinstance(te, np.ndarray):\n",
    "                te_t = to_torch_tensor(te, device=device)\n",
    "            else:\n",
    "                te_t = te.to(device)\n",
    "            out.append(ensure_2d(te_t.float()))\n",
    "        return torch.cat(out, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "617cc405",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_texts_for_audio(\n",
    "    model,\n",
    "    audio_path: str,\n",
    "    candidate_texts: List[str],\n",
    "    top_k: int = 5,\n",
    "    batch_size: int = 256,   # controls memory usage\n",
    "    device=None\n",
    ") -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Rank candidate_texts by cosine similarity to audio_path. Returns top_k tuples (text, score).\n",
    "    Score is cosine similarity in [-1,1].\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # ---- Audio embedding ----\n",
    "    audio_emb = embed_audio_single(model, audio_path, device=device)    # (1, D)\n",
    "    audio_n = l2_normalize(audio_emb)  # normalize\n",
    "\n",
    "    all_scores = []\n",
    "    all_texts = []\n",
    "\n",
    "    # ---- Process text embeddings in batches ----\n",
    "    for start in range(0, len(candidate_texts), batch_size):\n",
    "        end = start + batch_size\n",
    "        batch_texts = candidate_texts[start:end]\n",
    "\n",
    "        text_emb = embed_texts(model, batch_texts, device=device)   # (B, D)\n",
    "        text_n = l2_normalize(text_emb)\n",
    "\n",
    "        # similarity (1, B) â†’ (B,)\n",
    "        sims = (audio_n @ text_n.T).squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "        all_scores.extend(sims.tolist())\n",
    "        all_texts.extend(batch_texts)\n",
    "\n",
    "        # free GPU memory after each batch\n",
    "        del text_emb, text_n, sims\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # ---- Rank across all texts ----\n",
    "    all_scores = np.array(all_scores)\n",
    "    top_idx = np.argsort(-all_scores)[:top_k]\n",
    "\n",
    "    return [(all_texts[i], float(all_scores[i])) for i in top_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a4a839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_path = \"../Main_DataSet_Creation/data/audio/00/16000.mp3\"   # your path\n",
    "captions_df = pd.read_csv(\"../Main_DataSet_Creation/data/captions_checkpoint.csv\")  # or your caption file\n",
    "candidate_texts = captions_df[\"caption\"].astype(str).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b76bf3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc, torch\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "250bd061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |   1015 MiB |   1060 MiB |   1501 MiB | 497584 KiB |\n",
      "|       from large pool |    896 MiB |    914 MiB |   1069 MiB | 177550 KiB |\n",
      "|       from small pool |    119 MiB |    164 MiB |    431 MiB | 320033 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |   1015 MiB |   1060 MiB |   1501 MiB | 497584 KiB |\n",
      "|       from large pool |    896 MiB |    914 MiB |   1069 MiB | 177550 KiB |\n",
      "|       from small pool |    119 MiB |    164 MiB |    431 MiB | 320033 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |   1007 MiB |   1052 MiB |   1487 MiB | 492385 KiB |\n",
      "|       from large pool |    888 MiB |    906 MiB |   1056 MiB | 172389 KiB |\n",
      "|       from small pool |    119 MiB |    164 MiB |    431 MiB | 319995 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |   1102 MiB |   1102 MiB |   1102 MiB |      0 B   |\n",
      "|       from large pool |    932 MiB |    932 MiB |    932 MiB |      0 B   |\n",
      "|       from small pool |    170 MiB |    170 MiB |    170 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  25380 KiB |  55594 KiB |   1157 MiB |   1133 MiB |\n",
      "|       from large pool |  22400 KiB |  55040 KiB |    748 MiB |    726 MiB |\n",
      "|       from small pool |   2980 KiB |   6350 KiB |    409 MiB |    406 MiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     811    |    1007    |    1510    |     699    |\n",
      "|       from large pool |     183    |     186    |     244    |      61    |\n",
      "|       from small pool |     628    |     824    |    1266    |     638    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     811    |    1007    |    1510    |     699    |\n",
      "|       from large pool |     183    |     186    |     244    |      61    |\n",
      "|       from small pool |     628    |     824    |    1266    |     638    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     127    |     127    |     127    |       0    |\n",
      "|       from large pool |      42    |      42    |      42    |       0    |\n",
      "|       from small pool |      85    |      85    |      85    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      11    |      33    |     534    |     523    |\n",
      "|       from large pool |       4    |      21    |      90    |      86    |\n",
      "|       from small pool |       7    |      29    |     444    |     437    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a7d18e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top matches among captions:\n",
      "0.5054  A vigorous and aggressive instrumental Hard Rock piece with a loud electric guitar and drums playing alongside a deep male vocal roar. Keywords: hard rock, aggressive, instrumental, vigorous.\n",
      "0.5015  A fast and energetic rock piece with drums, guitar, electric guitar, and synthesizer sounds. Electric guitar solo, distorted, heavy, loud, and melodic. Key is E major, time signature is 4/4, tempo is 157.89\n",
      "0.4952  A fast-paced, aggressive instrumental rock piece with drums and electric guitar playing a loud, distorted beat. Keywords: rock, aggressive, instrumental, fast-paced.\n",
      "0.4947  A fast-paced, aggressive instrumental rock piece with drums and electric guitar playing a hard-hitting beat and a distorted bass line. Keywords: rock, aggressive, instrumental, fast-paced.\n",
      "0.4915  A passionate electronic rock piece with heavy drums and distorted guitars playing an energetic melody. Keywords: electronic, rock, drums, guitar.\n",
      "0.4909  A loud, aggressive instrumental rock piece with drums and electric guitars playing a fast beat and harmony. Keywords: rock, aggressive, instrumental, loud.\n",
      "0.4903  A passionate male vocalist sings this rock melody with a distorted electric guitar and groovy bass line over punchy drums and synthesizer sounds. The genre is rock or alternative. Keywords: Rock, Alternative, Male Vocals\n",
      "0.4874  A fast-paced, loud instrumental rock piece with drums and electric guitar playing a complex beat and solo respectively. Keywords: rock, instrumental, loud, fast-paced.\n",
      "0.4866  A fast and aggressive instrumental rock piece with distorted electric guitar, heavy drums, and keyboard harmony. Keywords: Rock, Instrumental, Aggressive, Fast.\n",
      "0.4850  A fast and energetic rock piece with drums, guitar, bass, and electric guitar playing a loud and melodic tune. Keywords: rock, drums, guitar, bass.\n"
     ]
    }
   ],
   "source": [
    "top = rank_texts_for_audio(model, audio_path, candidate_texts, top_k=10, device=device, batch_size=256)\n",
    "print(\"Top matches among captions:\")\n",
    "for t, s in top:\n",
    "    print(f\"{s:.4f}  {t[:220].replace('/n',' ')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85bdd86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "clap_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
